{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TucHczExm-Ah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  7 14:40:28 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   30C    P8    19W / 350W |     19MiB / 24265MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1273      G   /usr/lib/xorg/Xorg                  9MiB |\r\n",
      "|    0   N/A  N/A      1442      G   /usr/bin/gnome-shell                8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBc8Ztvtljau"
   },
   "source": [
    "## 필요 패키지 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdFUgVMemOrq"
   },
   "source": [
    "## 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bsfvgg9DmYsD"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from transformers import XLMRobertaModel, AutoTokenizer\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SIW3IcxmaPD"
   },
   "source": [
    "## 전역 변수 설정\n",
    "구글 드라이브 마운트 기준으로 설정되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rsmEKuymcxe",
    "outputId": "a0a87b81-540c-4d58-9249-c7a3b1fccf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'제품 전체#일반': 0, '제품 전체#가격': 1, '제품 전체#디자인': 2, '제품 전체#품질': 3, '제품 전체#편의성': 4, '제품 전체#인지도': 5, '제품 전체#다양성': 6, '본품#일반': 7, '본품#디자인': 8, '본품#품질': 9, '본품#편의성': 10, '본품#다양성': 11, '본품#가격': 12, '본품#인지도': 13, '패키지/구성품#일반': 14, '패키지/구성품#디자인': 15, '패키지/구성품#품질': 16, '패키지/구성품#편의성': 17, '패키지/구성품#다양성': 18, '패키지/구성품#가격': 19, '브랜드#일반': 20, '브랜드#가격': 21, '브랜드#디자인': 22, '브랜드#품질': 23, '브랜드#인지도': 24}\n"
     ]
    }
   ],
   "source": [
    "PADDING_TOKEN = 1\n",
    "S_OPEN_TOKEN = 0\n",
    "S_CLOSE_TOKEN = 2\n",
    "\n",
    "do_eval=False\n",
    "\n",
    "category_extraction_model_path = './saved_model/category_total_64/' \n",
    "polarity_classification_model_path = './saved_model/polarity_total_64/'\n",
    "\n",
    "test_category_extraction_model_path = './saved_model/category_total_64/saved_model_epoch_14.pt'\n",
    "test_polarity_classification_model_path = './saved_model/polarity_total_64/saved_model_epoch_14.pt'\n",
    "\n",
    "train_data_path = './data/nikluge-sa-2022-total.jsonl'\n",
    "dev_data_path = './data/nikluge-sa-2022-dev.jsonl'\n",
    "test_data_path = './data/nikluge-sa-2022-test.jsonl'\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 32\n",
    "base_model = 'monologg/koelectra-base-v3-discriminator'\n",
    "learning_rate = 1e-6\n",
    "eps = 1e-8\n",
    "num_train_epochs = 20\n",
    "classifier_hidden_size = 768\n",
    "emb_classifier_hidden_size = 1280 # 기존 768에서 -> 1024 임베딩을 concat한 vector\n",
    "classifier_dropout_prob = 0.1\n",
    "lstm_hidden = 256\n",
    "lstm_num_layer = 1\n",
    "hidden_dropout_prob = 0.3\n",
    "bilstm_flag = True\n",
    "\n",
    "entity_property_pair = [\n",
    "    '제품 전체#일반', '제품 전체#가격', '제품 전체#디자인', '제품 전체#품질', '제품 전체#편의성', '제품 전체#인지도', '제품 전체#다양성',\n",
    "    '본품#일반', '본품#디자인', '본품#품질', '본품#편의성', '본품#다양성', '본품#가격', '본품#인지도',\n",
    "    '패키지/구성품#일반', '패키지/구성품#디자인', '패키지/구성품#품질', '패키지/구성품#편의성', '패키지/구성품#다양성', '패키지/구성품#가격',\n",
    "    '브랜드#일반', '브랜드#가격', '브랜드#디자인', '브랜드#품질', '브랜드#인지도' ]\n",
    "\n",
    "entity_property_to_id = {entity_property_pair[i] : i for i in range(len(entity_property_pair))}\n",
    "print(entity_property_to_id)\n",
    "\n",
    "tf_id_to_name = ['True', 'False']\n",
    "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
    "\n",
    "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
    "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
    "}\n",
    "\n",
    "entity_property_seq = [ i for i in range(len(entity_property_pair))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioSV9b9am0UO"
   },
   "source": [
    "## Json 파일 읽어오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkSVE1qxm25-",
    "outputId": "21048907-214a-445e-82d8-94db3ade73eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-sa-2022-train-00001',\n",
       "  'sentence_form': '둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.',\n",
       "  'annotation': [['본품#품질', ['기어', 16, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00002',\n",
       "  'sentence_form': '이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.',\n",
       "  'annotation': [['본품#품질', ['기어 텐션', 67, 72], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00003',\n",
       "  'sentence_form': '간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00004',\n",
       "  'sentence_form': '샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.',\n",
       "  'annotation': [['제품 전체#일반', ['샥이 없는 모델', 0, 8], 'neutral']]},\n",
       " {'id': 'nikluge-sa-2022-train-00005',\n",
       "  'sentence_form': '안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.',\n",
       "  'annotation': [['본품#일반', ['안장', 0, 2], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00006',\n",
       "  'sentence_form': '지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..',\n",
       "  'annotation': [['제품 전체#일반', ['자전거', 23, 26], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00007',\n",
       "  'sentence_form': '내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.',\n",
       "  'annotation': [['본품#품질', ['내장 기어 3단', 0, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00008',\n",
       "  'sentence_form': '한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ',\n",
       "  'annotation': [['본품#품질', ['UD20', 14, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00009',\n",
       "  'sentence_form': '전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!',\n",
       "  'annotation': [['본품#품질', ['자막 검색 후 등록 기능', 11, 24], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00010',\n",
       "  'sentence_form': '왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!',\n",
       "  'annotation': [['본품#품질', ['등록]키', 3, 7], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00011',\n",
       "  'sentence_form': '다른 부가 기능은 참 훌륭한데..',\n",
       "  'annotation': [['본품#품질', ['부가 기능', 3, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00012',\n",
       "  'sentence_form': '미치겠네.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00013',\n",
       "  'sentence_form': '아.. 진짜 기계 사겠나.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00014',\n",
       "  'sentence_form': '이번에는 사전까지..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00015',\n",
       "  'sentence_form': '이런 젠장..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list\n",
    "\n",
    "jsonlload('./data/sample.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8E_7iJam7wO"
   },
   "source": [
    "## 모델 정의\n",
    "xlm-roberta 모델을 기반으로 한 classification 모델 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OLIjuYxmopZX"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(classifier_hidden_size, classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(classifier_dropout_prob)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_classifier_hidden_size, classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(classifier_dropout_prob)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class CategoryClassification(nn.Module):\n",
    "    def __init__(self, num_label, len_tokenizer, category_emb_size, category_size, num_layer, bilstm_flag):\n",
    "        super().__init__()\n",
    "\n",
    "        assert category_emb_size == lstm_hidden * 2, \"Please set category-embedding-size to twice the lstm-hidden-size\"\n",
    "\n",
    "        self.num_label = num_label # 0, 1 -> 0이면 해당 카테고리는 있는 거임.\n",
    "        self.electra = ElectraModel.from_pretrained(base_model)\n",
    "        self.electra.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.n_hidden = lstm_hidden\n",
    "\n",
    "        self.category_emb = nn.Embedding(category_size, category_emb_size, scale_grad_by_freq=True)\n",
    "\n",
    "        self.num_layers = num_layer\n",
    "        self.bidirectional = 2 if bilstm_flag else 1\n",
    "\n",
    "        self.category_lstm_first = nn.LSTM(768, 256, bidirectional=True, batch_first=True)\n",
    "        self.category_lstm_last = nn.LSTM(lstm_hidden * 4, self.n_hidden, num_layers=self.num_layers, batch_first=True, bidirectional=bilstm_flag)\n",
    "\n",
    "        self.category_q_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "        self.category_k_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "        self.category_v_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.labels_classifier = EmbeddingClassifier(self.num_label) # 입력 사이즈는 classifier_hidden_size = (768, 2) 0과 1 -> \n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, \n",
    "                category_label_seq_tensor=None, category_one_hot=None, token_type_ids=None, ):\n",
    "\n",
    "        outputs = self.electra(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        # (batch_size, max_length, hidden_size)\n",
    "        sequence_output = outputs[0] # (batch_size, 256, 768)\n",
    "        # print(\"sequence_output size:\", sequence_output.shape)\n",
    "\n",
    "        category_embs = self.category_emb(category_label_seq_tensor)\n",
    "        # print(category_label_seq_tensor)\n",
    "        # print('category_embs size:', category_embs.shape)\n",
    "\n",
    "        hidden = None\n",
    "        scaler = self.n_hidden ** 0.5\n",
    "\n",
    "        \"\"\"\n",
    "        category predict layer\n",
    "        \"\"\"\n",
    "        category_lstm_outputs, hidden = self.category_lstm_first(sequence_output, hidden)\n",
    "        category_lstm_outputs = self.dropout(category_lstm_outputs)\n",
    "\n",
    "        # print('category_lstm_outputs size:',category_lstm_outputs.shape)\n",
    "\n",
    "        category_q = self.category_q_liner(category_lstm_outputs)\n",
    "        category_k = self.category_k_liner(category_embs)\n",
    "        category_v = self.category_v_liner(category_embs)\n",
    "\n",
    "        # print(\"category_q\", category_q.shape)\n",
    "        # print(\"category_k\", category_k.shape)\n",
    "        # print(\"category_v\", category_v.shape)\n",
    "\n",
    "        #category_attention_score = category_q.matmul(category_k.permute(0, 2, 1)) / scaler\n",
    "        #category_attention_align = self.softmax(category_attention_score)\n",
    "        \n",
    "        # print(\"category_one_hot\", category_one_hot.shape)\n",
    "        \n",
    "        category_attention_align = category_one_hot[0].expand(category_one_hot.shape[0], max_len, len(entity_property_pair))\n",
    "        category_attention_align = category_attention_align.float()\n",
    "        # print(\"category_attention_score\", category_attention_score)\n",
    "        # print(category_attention_align.dtype)\n",
    "        # print(\"category_attention_align\", category_attention_align.shape)\n",
    "\n",
    "        category_attention_output = category_attention_align.matmul(category_v)\n",
    "        category_attention_output = self.dropout(category_attention_output)\n",
    "        # print(\"category_attention_output\", category_attention_output.shape)\n",
    "\n",
    "\n",
    "        category_lstm_outputs = torch.cat([category_lstm_outputs, category_attention_output], dim=-1)\n",
    "        # print(\"category_lstm_outputs:\", category_lstm_outputs.shape)\n",
    "\n",
    "        category_lstm_outputs, hidden = self.category_lstm_last(category_lstm_outputs, hidden)\n",
    "        category_lstm_outputs = self.dropout(category_lstm_outputs) # 요걸 다음층에 입력에 넣어야 하나..?\n",
    "\n",
    "        # print(\"category_lstm_outputs\", category_lstm_outputs.shape)\n",
    "\n",
    "        category_q = self.category_q_liner(category_lstm_outputs)\n",
    "        category_k = self.category_k_liner(category_embs)\n",
    "\n",
    "        # print(\"category_q_last\", category_q.shape)\n",
    "        # print(\"category_k_last\", category_k.shape)\n",
    "\n",
    "        category_attention_score = category_q.matmul(category_k.permute(0, 2, 1)) / scaler\n",
    "        category_attention_score = self.dropout(category_attention_score)\n",
    "        \n",
    "        # print(\"category_attention_score\", category_attention_score.shape)\n",
    "\n",
    "        #final_category_attention_score = category_attention_score[:, 0, :]\n",
    "        final_category_attention_score = category_one_hot\n",
    "        # print(\"final_category_attention_score\", final_category_attention_score.shape)\n",
    "\n",
    "        category_attention_score = category_attention_score.matmul(category_embs) # [batch_size, max_length, max_length]\n",
    "        category_attention_score = torch.cat([sequence_output, category_attention_score], dim=-1)\n",
    "\n",
    "        # print(category_attention_score.shape)\n",
    "      \n",
    "        logits = self.labels_classifier(category_attention_score)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "class RoBertaBaseClassifier(nn.Module):\n",
    "    def __init__(self, num_label, len_tokenizer):\n",
    "        super(RoBertaBaseClassifier, self).__init__()\n",
    "\n",
    "        self.num_label = num_label\n",
    "        self.electra = ElectraModel.from_pretrained(base_model)\n",
    "        self.electra.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.labels_classifier = SimpleClassifier(self.num_label)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        # print(sequence_output.shape)\n",
    "        logits = self.labels_classifier(sequence_output)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xu_9i_aoreS"
   },
   "source": [
    "## 데이터 파싱 및 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index))\n",
    "       index = word2index[word]\n",
    "       one_hot_vector[index] = 1\n",
    "       return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = one_hot_encoding('제품 전체#디자인', entity_property_to_id)\n",
    "# sample = torch.tensor(sample)\n",
    "# sample = sample.expand(8,25)\n",
    "# sample = sample.expand(8, 256)\n",
    "# print(sample)\n",
    "# print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fA7HcyNWq8ui"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
    "\n",
    "    entity_property_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': [],\n",
    "        'category': [],\n",
    "        'category_label': []\n",
    "    }\n",
    "    polarity_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    for pair in entity_property_pair:\n",
    "        isPairInOpinion = False\n",
    "        if pd.isna(form):\n",
    "            break\n",
    "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
    "        for annotation in annotations:\n",
    "            entity_property = annotation[0]\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if polarity == '------------':\n",
    "                continue\n",
    "\n",
    "            category_seq = [ i for i in range(len(entity_property_pair))] \n",
    "            category_one_hot = one_hot_encoding(pair,entity_property_to_id )\n",
    "\n",
    "            if entity_property == pair:\n",
    "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
    "                entity_property_data_dict['category'].append(category_seq)\n",
    "                entity_property_data_dict['category_label'].append(category_one_hot)\n",
    "                # print(entity_property_data_dict)\n",
    "\n",
    "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
    "\n",
    "                isPairInOpinion = True\n",
    "                break\n",
    "\n",
    "        if isPairInOpinion is False:\n",
    "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
    "            entity_property_data_dict['category'].append(category_seq) # 개체 없음(즉, 틀린 개체#속성 pair의 경우)\n",
    "            entity_property_data_dict['category_label'].append(category_one_hot)\n",
    "\n",
    "    return entity_property_data_dict, polarity_data_dict\n",
    "\n",
    "\n",
    "def get_dataset(raw_data, tokenizer, max_len):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_labels_list = []\n",
    "    category_seq_list = []\n",
    "    category_one_hot_list = []\n",
    "\n",
    "    polarity_input_ids_list = []\n",
    "    polarity_attention_mask_list = []\n",
    "    polarity_token_labels_list = []\n",
    "\n",
    "    for utterance in raw_data:\n",
    "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
    "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
    "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
    "        token_labels_list.extend(entity_property_data_dict['label'])\n",
    "        category_seq_list.extend(entity_property_data_dict['category'])\n",
    "        category_one_hot_list.extend(entity_property_data_dict['category_label'])\n",
    "       \n",
    "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
    "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
    "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
    "\n",
    "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
    "                         torch.tensor(token_labels_list), torch.tensor(category_seq_list),\n",
    "                        torch.tensor(category_one_hot_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
    "                         torch.tensor(polarity_token_labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmGcKTDLrD5-"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oV8CmkrCrJAC"
   },
   "outputs": [],
   "source": [
    "def evaluation(y_true, y_pred, label_len):\n",
    "    count_list = [0]*label_len\n",
    "    hit_list = [0]*label_len\n",
    "    for i in range(len(y_true)):\n",
    "        count_list[y_true[i]] += 1\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            hit_list[y_true[i]] += 1\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(label_len):\n",
    "        acc_list.append(hit_list[i]/count_list[i])\n",
    "\n",
    "    print(count_list)\n",
    "    print(hit_list)\n",
    "    print(acc_list)\n",
    "    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n",
    "    print('macro_accuracy: ', sum(acc_list) / 3)\n",
    "    # print(y_true)\n",
    "\n",
    "    y_true = list(map(int, y_true))\n",
    "    y_pred = list(map(int, y_pred))\n",
    "\n",
    "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
    "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
    "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "def train_sentiment_analysis():\n",
    "\n",
    "    print('train_sentiment_analysis')\n",
    "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
    "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
    "\n",
    "    print('loading train data...')\n",
    "    train_data = jsonlload(train_data_path)\n",
    "    dev_data = jsonlload(dev_data_path)\n",
    "\n",
    "    print('tokenizing train data...')\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "    print('making dataset...')\n",
    "    entity_property_train_data, polarity_train_data = get_dataset(train_data, tokenizer, max_len)\n",
    "    entity_property_dev_data, polarity_dev_data = get_dataset(dev_data, tokenizer, max_len)\n",
    "\n",
    "    print('making dataloader...')\n",
    "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
    "                                  batch_size=batch_size)\n",
    "    entity_property_dev_dataloader = DataLoader(entity_property_dev_data, shuffle=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "    polarity_dev_dataloader = DataLoader(polarity_dev_data, shuffle=True,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print('loading model...')\n",
    "    # entity_property_model = RoBertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
    "    entity_property_model = CategoryClassification(len(tf_id_to_name), len(tokenizer), lstm_hidden * 2, len(entity_property_pair), lstm_num_layer, bilstm_flag)\n",
    "    entity_property_model.to(device)\n",
    "\n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.to(device)\n",
    "\n",
    "    print('end loading')\n",
    "\n",
    "    # entity_property_model_optimizer_setting\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        entity_property_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
    "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
    "\n",
    "    entity_property_optimizer = AdamW(\n",
    "        entity_property_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(entity_property_train_dataloader)\n",
    "    print(\"total_steps : \", total_steps)\n",
    "\n",
    "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
    "        entity_property_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # polarity_model_optimizer_setting\n",
    "    if FULL_FINETUNING:\n",
    "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        polarity_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
    "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
    "\n",
    "    polarity_optimizer = AdamW(\n",
    "        polarity_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(polarity_train_dataloader)\n",
    "\n",
    "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
    "        polarity_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    epoch_step = 0\n",
    "    print(\"학습을 시작합니다...\")\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        entity_property_model.train()\n",
    "        epoch_step += 1\n",
    "\n",
    "        # entity_property train\n",
    "        entity_property_total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(entity_property_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels, b_category_seq, b_category_one_hot = batch\n",
    "            # print(\"b_input_ids:\", b_input_ids)\n",
    "            # print(\"b_input_mask:\", b_input_mask)\n",
    "            # print(\"b_labels:\", b_labels)\n",
    "            # print(\"b_category_one_hot:\", b_category_one_hot.shape)\n",
    "\n",
    "            entity_property_model.zero_grad()\n",
    "\n",
    "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels, b_category_seq, b_category_one_hot)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            entity_property_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
    "            entity_property_optimizer.step()\n",
    "            entity_property_scheduler.step()\n",
    "\n",
    "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = category_extraction_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            entity_property_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in entity_property_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels, b_category_seq, b_category_one_hot = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = entity_property_model(b_input_ids, b_input_mask, b_labels, b_category_seq, b_category_one_hot)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(tf_id_to_name))\n",
    "\n",
    "\n",
    "        # polarity train\n",
    "        polarity_total_loss = 0\n",
    "        polarity_model.train()\n",
    "\n",
    "        for step, batch in enumerate(polarity_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            polarity_model.zero_grad()\n",
    "\n",
    "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            polarity_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
    "            polarity_optimizer.step()\n",
    "            polarity_scheduler.step()\n",
    "\n",
    "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = polarity_classification_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            polarity_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in polarity_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(polarity_id_to_name))\n",
    "\n",
    "    print(\"training is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "q85k-NgDrLo9",
    "outputId": "61d00fd1-e732-47f8-c08e-3c9a77b09a02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentiment_analysis\n",
      "category_extraction model would be saved at  ./saved_model/category_total_64/\n",
      "polarity model would be saved at  ./saved_model/polarity_total_64/\n",
      "loading train data...\n",
      "tokenizing train data...\n",
      "We have added 8 tokens\n",
      "making dataset...\n",
      "making dataloader...\n",
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end loading\n",
      "total_steps :  90560\n",
      "학습을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  1\n",
      "Average train loss: 0.15089906916160517\n",
      "Entity_Property_Epoch:  1\n",
      "Average train loss: 0.8293836196673285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [20:51<6:36:22, 1251.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  2\n",
      "Average train loss: 0.10679356343374717\n",
      "Entity_Property_Epoch:  2\n",
      "Average train loss: 0.38040559784960504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [41:44<6:15:43, 1252.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  3\n",
      "Average train loss: 0.09508385528853927\n",
      "Entity_Property_Epoch:  3\n",
      "Average train loss: 0.23273232183659198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [1:02:37<5:54:52, 1252.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  4\n",
      "Average train loss: 0.08771196645138458\n",
      "Entity_Property_Epoch:  4\n",
      "Average train loss: 0.2031184691436512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [1:23:29<5:33:58, 1252.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  5\n",
      "Average train loss: 0.08303348159848102\n",
      "Entity_Property_Epoch:  5\n",
      "Average train loss: 0.18425121742118264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [1:44:21<5:13:05, 1252.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  6\n",
      "Average train loss: 0.07973131196441843\n",
      "Entity_Property_Epoch:  6\n",
      "Average train loss: 0.16510053286232898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [2:05:14<4:52:13, 1252.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  7\n",
      "Average train loss: 0.07650933931075958\n",
      "Entity_Property_Epoch:  7\n",
      "Average train loss: 0.15562707514108456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [2:26:06<4:31:20, 1252.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  8\n",
      "Average train loss: 0.07346442703932514\n",
      "Entity_Property_Epoch:  8\n",
      "Average train loss: 0.1505719545230116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 8/20 [2:46:58<4:10:26, 1252.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  9\n",
      "Average train loss: 0.07170300591221185\n",
      "Entity_Property_Epoch:  9\n",
      "Average train loss: 0.147651979776541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|████▌     | 9/20 [3:07:50<3:49:32, 1252.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  10\n",
      "Average train loss: 0.07079128037382326\n",
      "Entity_Property_Epoch:  10\n",
      "Average train loss: 0.14215706741994188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 10/20 [3:28:41<3:28:38, 1251.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  11\n",
      "Average train loss: 0.06965009584138396\n",
      "Entity_Property_Epoch:  11\n",
      "Average train loss: 0.13865348344342304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████▌    | 11/20 [3:49:32<3:07:45, 1251.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  12\n",
      "Average train loss: 0.0673938717995197\n",
      "Entity_Property_Epoch:  12\n",
      "Average train loss: 0.1373783061952935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 12/20 [4:10:24<2:46:53, 1251.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  13\n",
      "Average train loss: 0.06750833277832556\n",
      "Entity_Property_Epoch:  13\n",
      "Average train loss: 0.13573006718160258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|██████▌   | 13/20 [4:31:16<2:26:01, 1251.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  14\n",
      "Average train loss: 0.06601139076107754\n",
      "Entity_Property_Epoch:  14\n",
      "Average train loss: 0.1362387148240946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [4:56:21<2:07:00, 1270.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5ca2a3016759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sentiment_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-1fb795bb4edb>\u001b[0m in \u001b[0;36mtrain_sentiment_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mentity_property_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_property_train_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_category_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_category_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docdoca_kisti/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docdoca_kisti/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docdoca_kisti/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docdoca_kisti/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docdoca_kisti/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5gk03mSr6T8"
   },
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tUpzE4G4r7kv"
   },
   "outputs": [],
   "source": [
    "def predict_from_korean_form(tokenizer, ce_model, pc_model, data):\n",
    "\n",
    "    ce_model.to(device)\n",
    "    ce_model.eval()\n",
    "    for sentence in data:\n",
    "        form = sentence['sentence_form']\n",
    "        sentence['annotation'] = []\n",
    "        if type(form) != str:\n",
    "            print(\"form type is arong: \", form)\n",
    "            continue\n",
    "        for pair in entity_property_pair:\n",
    "            \n",
    "            tokenized_data = tokenizer(form, pair, padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "            input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
    "            attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
    "            category_seq = [ i for i in range(len(entity_property_pair))] \n",
    "            category_seq = torch.tensor([category_seq]).to(device)\n",
    "            category_one_hot = one_hot_encoding(pair, entity_property_to_id)\n",
    "            category_one_hot = torch.tensor([category_one_hot]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, ce_logits = ce_model(input_ids, attention_mask, category_label_seq_tensor=category_seq,category_one_hot=category_one_hot)\n",
    "\n",
    "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
    "\n",
    "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
    "\n",
    "            if ce_result == 'True':\n",
    "                with torch.no_grad():\n",
    "                    _, pc_logits = pc_model(input_ids, attention_mask)\n",
    "\n",
    "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
    "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
    "\n",
    "                sentence['annotation'].append([pair, pc_result])\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH0tDklIr_I1"
   },
   "source": [
    "## F1 score 계산 - 추출 성능 및 전체 성능에 대한 F1 score 따로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OAnHXy1QsEEl"
   },
   "outputs": [],
   "source": [
    "def evaluation_f1(true_data, pred_data):\n",
    "\n",
    "    true_data_list = true_data\n",
    "    pred_data_list = pred_data\n",
    "\n",
    "    ce_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    pipeline_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    for i in range(len(true_data_list)):\n",
    "\n",
    "        # TP, FN checking\n",
    "        is_ce_found = False\n",
    "        is_pipeline_found = False\n",
    "        for y_ano  in true_data_list[i]['annotation']:\n",
    "            y_category = y_ano[0]\n",
    "            y_polarity = y_ano[2]\n",
    "\n",
    "            for p_ano in pred_data_list[i]['annotation']:\n",
    "                p_category = p_ano[0]\n",
    "                p_polarity = p_ano[1]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is True:\n",
    "                ce_eval['TP'] += 1\n",
    "            else:\n",
    "                ce_eval['FN'] += 1\n",
    "\n",
    "            if is_pipeline_found is True:\n",
    "                pipeline_eval['TP'] += 1\n",
    "            else:\n",
    "                pipeline_eval['FN'] += 1\n",
    "\n",
    "            is_ce_found = False\n",
    "            is_pipeline_found = False\n",
    "\n",
    "        # FP checking\n",
    "        for p_ano in pred_data_list[i]['annotation']:\n",
    "            p_category = p_ano[0]\n",
    "            p_polarity = p_ano[1]\n",
    "\n",
    "            for y_ano  in true_data_list[i]['annotation']:\n",
    "                y_category = y_ano[0]\n",
    "                y_polarity = y_ano[2]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is False:\n",
    "                ce_eval['FP'] += 1\n",
    "\n",
    "            if is_pipeline_found is False:\n",
    "                pipeline_eval['FP'] += 1\n",
    "\n",
    "    ce_precision = ce_eval['TP']/(ce_eval['TP']+ce_eval['FP'])\n",
    "    ce_recall = ce_eval['TP']/(ce_eval['TP']+ce_eval['FN'])\n",
    "\n",
    "    ce_result = {\n",
    "        'Precision': ce_precision,\n",
    "        'Recall': ce_recall,\n",
    "        'F1': 2*ce_recall*ce_precision/(ce_recall+ce_precision)\n",
    "    }\n",
    "\n",
    "    pipeline_precision = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FP'])\n",
    "    pipeline_recall = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FN'])\n",
    "\n",
    "    pipeline_result = {\n",
    "        'Precision': pipeline_precision,\n",
    "        'Recall': pipeline_recall,\n",
    "        'F1': 2*pipeline_recall*pipeline_precision/(pipeline_recall+pipeline_precision)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'category extraction result': ce_result,\n",
    "        'entire pipeline result': pipeline_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuiak9l3sGLf"
   },
   "source": [
    "## 테스트 데이터에 대한 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xfKEPk06sIuU"
   },
   "outputs": [],
   "source": [
    "def test_sentiment_analysis():\n",
    "\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    test_data = jsonlload(test_data_path)\n",
    "\n",
    "#     entity_property_test_data, polarity_test_data = get_dataset(test_data, tokenizer, max_len)\n",
    "\n",
    "#     entity_property_test_dataloader = DataLoader(entity_property_test_data, shuffle=True,\n",
    "#                                 batch_size=batch_size)\n",
    "\n",
    "#     polarity_test_dataloader = DataLoader(polarity_test_data, shuffle=True,\n",
    "#                                                   batch_size=batch_size)\n",
    "    \n",
    "    model =  CategoryClassification(len(tf_id_to_name), len(tokenizer), lstm_hidden * 2, len(entity_property_pair), lstm_num_layer, bilstm_flag)\n",
    "    model.load_state_dict(torch.load(test_category_extraction_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "            \n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
    "    polarity_model.to(device)\n",
    "    polarity_model.eval()\n",
    "\n",
    "    pred_data = predict_from_korean_form(tokenizer, model, polarity_model, copy.deepcopy(test_data))\n",
    "\n",
    "    jsondump(pred_data, './pred_data.json')\n",
    "    pred_data = jsonload('./pred_data.json')\n",
    "\n",
    "    print('F1 result: ', evaluation_f1(test_data, pred_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Pe79889ksMNX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5b1156452cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sentiment_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4b457f46e37c>\u001b[0m in \u001b[0;36mtest_sentiment_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpred_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pred_data.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 result: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-515cdf4791c7>\u001b[0m in \u001b[0;36mevaluation_f1\u001b[0;34m(true_data, pred_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mce_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mce_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mce_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     ce_result = {\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "test_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NNjZffvnczK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "docdoca_kisti",
   "language": "python",
   "name": "docdoca_kisti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
