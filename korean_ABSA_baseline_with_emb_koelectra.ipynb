{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TucHczExm-Ah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 30 21:17:23 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   29C    P8     9W / 350W |     17MiB / 24265MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1273      G   /usr/lib/xorg/Xorg                  9MiB |\r\n",
      "|    0   N/A  N/A      1442      G   /usr/bin/gnome-shell                6MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBc8Ztvtljau"
   },
   "source": [
    "## 필요 패키지 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdFUgVMemOrq"
   },
   "source": [
    "## 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bsfvgg9DmYsD"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from transformers import XLMRobertaModel, AutoTokenizer\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SIW3IcxmaPD"
   },
   "source": [
    "## 전역 변수 설정\n",
    "구글 드라이브 마운트 기준으로 설정되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rsmEKuymcxe",
    "outputId": "a0a87b81-540c-4d58-9249-c7a3b1fccf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'제품 전체#일반': 0, '제품 전체#가격': 1, '제품 전체#디자인': 2, '제품 전체#품질': 3, '제품 전체#편의성': 4, '제품 전체#인지도': 5, '제품 전체#다양성': 6, '본품#일반': 7, '본품#디자인': 8, '본품#품질': 9, '본품#편의성': 10, '본품#다양성': 11, '본품#가격': 12, '본품#인지도': 13, '패키지/구성품#일반': 14, '패키지/구성품#디자인': 15, '패키지/구성품#품질': 16, '패키지/구성품#편의성': 17, '패키지/구성품#다양성': 18, '패키지/구성품#가격': 19, '브랜드#일반': 20, '브랜드#가격': 21, '브랜드#디자인': 22, '브랜드#품질': 23, '브랜드#인지도': 24}\n"
     ]
    }
   ],
   "source": [
    "PADDING_TOKEN = 1\n",
    "S_OPEN_TOKEN = 0\n",
    "S_CLOSE_TOKEN = 2\n",
    "\n",
    "do_eval=True\n",
    "\n",
    "category_extraction_model_path = './saved_model/category_extraction_emb/' \n",
    "polarity_classification_model_path = './saved_model/polarity_classification_emb/'\n",
    "\n",
    "test_category_extraction_model_path = './saved_model/category_extraction_emb/saved_model_epoch_15.pt'\n",
    "test_polarity_classification_model_path = './saved_model/polarity_classification_emb/saved_model_epoch_15.pt'\n",
    "\n",
    "train_data_path = './data/nikluge-sa-2022-train.jsonl'\n",
    "dev_data_path = './data/nikluge-sa-2022-dev.jsonl'\n",
    "test_data_path = './data/nikluge-sa-2022-test.jsonl'\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 8\n",
    "base_model = 'monologg/koelectra-base-v3-discriminator'\n",
    "learning_rate = 3e-6\n",
    "eps = 1e-8\n",
    "num_train_epochs = 20\n",
    "classifier_hidden_size = 768\n",
    "emb_classifier_hidden_size = 1280 # 기존 768에서 -> 1024 임베딩을 concat한 vector\n",
    "classifier_dropout_prob = 0.1\n",
    "lstm_hidden = 256\n",
    "lstm_num_layer = 1\n",
    "hidden_dropout_prob = 0.3\n",
    "bilstm_flag = True\n",
    "\n",
    "entity_property_pair = [\n",
    "    '제품 전체#일반', '제품 전체#가격', '제품 전체#디자인', '제품 전체#품질', '제품 전체#편의성', '제품 전체#인지도', '제품 전체#다양성',\n",
    "    '본품#일반', '본품#디자인', '본품#품질', '본품#편의성', '본품#다양성', '본품#가격', '본품#인지도',\n",
    "    '패키지/구성품#일반', '패키지/구성품#디자인', '패키지/구성품#품질', '패키지/구성품#편의성', '패키지/구성품#다양성', '패키지/구성품#가격',\n",
    "    '브랜드#일반', '브랜드#가격', '브랜드#디자인', '브랜드#품질', '브랜드#인지도' ]\n",
    "\n",
    "entity_property_to_id = {entity_property_pair[i] : i for i in range(len(entity_property_pair))}\n",
    "print(entity_property_to_id)\n",
    "\n",
    "tf_id_to_name = ['True', 'False']\n",
    "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
    "\n",
    "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
    "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
    "}\n",
    "\n",
    "entity_property_seq = [ i for i in range(len(entity_property_pair))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioSV9b9am0UO"
   },
   "source": [
    "## Json 파일 읽어오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkSVE1qxm25-",
    "outputId": "21048907-214a-445e-82d8-94db3ade73eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-sa-2022-train-00001',\n",
       "  'sentence_form': '둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.',\n",
       "  'annotation': [['본품#품질', ['기어', 16, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00002',\n",
       "  'sentence_form': '이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.',\n",
       "  'annotation': [['본품#품질', ['기어 텐션', 67, 72], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00003',\n",
       "  'sentence_form': '간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00004',\n",
       "  'sentence_form': '샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.',\n",
       "  'annotation': [['제품 전체#일반', ['샥이 없는 모델', 0, 8], 'neutral']]},\n",
       " {'id': 'nikluge-sa-2022-train-00005',\n",
       "  'sentence_form': '안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.',\n",
       "  'annotation': [['본품#일반', ['안장', 0, 2], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00006',\n",
       "  'sentence_form': '지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..',\n",
       "  'annotation': [['제품 전체#일반', ['자전거', 23, 26], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00007',\n",
       "  'sentence_form': '내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.',\n",
       "  'annotation': [['본품#품질', ['내장 기어 3단', 0, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00008',\n",
       "  'sentence_form': '한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ',\n",
       "  'annotation': [['본품#품질', ['UD20', 14, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00009',\n",
       "  'sentence_form': '전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!',\n",
       "  'annotation': [['본품#품질', ['자막 검색 후 등록 기능', 11, 24], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00010',\n",
       "  'sentence_form': '왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!',\n",
       "  'annotation': [['본품#품질', ['등록]키', 3, 7], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00011',\n",
       "  'sentence_form': '다른 부가 기능은 참 훌륭한데..',\n",
       "  'annotation': [['본품#품질', ['부가 기능', 3, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00012',\n",
       "  'sentence_form': '미치겠네.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00013',\n",
       "  'sentence_form': '아.. 진짜 기계 사겠나.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00014',\n",
       "  'sentence_form': '이번에는 사전까지..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00015',\n",
       "  'sentence_form': '이런 젠장..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list\n",
    "\n",
    "jsonlload('./data/sample.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8E_7iJam7wO"
   },
   "source": [
    "## 모델 정의\n",
    "xlm-roberta 모델을 기반으로 한 classification 모델 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OLIjuYxmopZX"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(classifier_hidden_size, classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(classifier_dropout_prob)\n",
    "        self.output = nn.Linear(classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_classifier_hidden_size, classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(classifier_dropout_prob)\n",
    "        self.output = nn.Linear(classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class CategoryClassification(nn.Module):\n",
    "    def __init__(self, num_label, len_tokenizer, category_emb_size, category_size, num_layer, bilstm_flag):\n",
    "        super().__init__()\n",
    "\n",
    "        assert category_emb_size == lstm_hidden * 2, \"Please set category-embedding-size to twice the lstm-hidden-size\"\n",
    "\n",
    "        self.num_label = num_label # 0, 1 -> 0이면 해당 카테고리는 있는 거임.\n",
    "        self.electra = ElectraModel.from_pretrained(base_model)\n",
    "        self.electra.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.n_hidden = lstm_hidden\n",
    "\n",
    "        self.category_emb = nn.Embedding(category_size, category_emb_size, scale_grad_by_freq=True)\n",
    "\n",
    "        self.num_layers = num_layer\n",
    "        self.bidirectional = 2 if bilstm_flag else 1\n",
    "\n",
    "        self.category_lstm_first = nn.LSTM(768, 256, bidirectional=True, batch_first=True)\n",
    "        self.category_lstm_last = nn.LSTM(lstm_hidden * 4, self.n_hidden, num_layers=self.num_layers, batch_first=True, bidirectional=bilstm_flag)\n",
    "\n",
    "        self.category_q_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "        self.category_k_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "        self.category_v_liner = nn.Linear(self.n_hidden * 2, self.n_hidden * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.labels_classifier = EmbeddingClassifier(self.num_label) # 입력 사이즈는 classifier_hidden_size = (768, 2) 0과 1 -> \n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, category_label_seq_tensor=None, token_type_ids=None ):\n",
    "\n",
    "        outputs = self.electra(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        # (batch_size, max_length, hidden_size)\n",
    "        sequence_output = outputs[0] # (batch_size, 256, 768)\n",
    "        # print(\"sequence_output size:\", sequence_output.shape)\n",
    "\n",
    "        category_embs = self.category_emb(category_label_seq_tensor)\n",
    "        # print(category_label_seq_tensor)\n",
    "        # print('category_embs size:', category_embs.shape)\n",
    "\n",
    "        hidden = None\n",
    "        scaler = self.n_hidden ** 0.5\n",
    "\n",
    "        \"\"\"\n",
    "        category predict layer\n",
    "        \"\"\"\n",
    "        category_lstm_outputs, hidden = self.category_lstm_first(sequence_output, hidden)\n",
    "        category_lstm_outputs = self.dropout(category_lstm_outputs)\n",
    "\n",
    "        # print('category_lstm_outputs size:',category_lstm_outputs.shape)\n",
    "\n",
    "        category_q = self.category_q_liner(category_lstm_outputs)\n",
    "        category_k = self.category_k_liner(category_embs)\n",
    "        category_v = self.category_v_liner(category_embs)\n",
    "\n",
    "        # print(\"category_q\", category_q.shape)\n",
    "        # print(\"category_k\", category_k.shape)\n",
    "        # print(\"category_v\", category_v.shape)\n",
    "\n",
    "        category_attention_score = category_q.matmul(category_k.permute(0, 2, 1)) / scaler\n",
    "        category_attention_align = self.softmax(category_attention_score)\n",
    "        # print(\"category_attention_score\", category_attention_score)\n",
    "\n",
    "        category_attention_output = category_attention_align.matmul(category_v)\n",
    "        category_attention_output = self.dropout(category_attention_output)\n",
    "        # print(\"category_attention_output\", category_attention_output.shape)\n",
    "\n",
    "\n",
    "        category_lstm_outputs = torch.cat([category_lstm_outputs, category_attention_output], dim=-1)\n",
    "        # print(\"category_lstm_outputs:\", category_lstm_outputs.shape)\n",
    "\n",
    "        category_lstm_outputs, hidden = self.category_lstm_last(category_lstm_outputs, hidden)\n",
    "        category_lstm_outputs = self.dropout(category_lstm_outputs) # 요걸 다음층에 입력에 넣어야 하나..?\n",
    "\n",
    "        # print(\"category_lstm_outputs\", category_lstm_outputs.shape)\n",
    "\n",
    "        category_q = self.category_q_liner(category_lstm_outputs)\n",
    "        category_k = self.category_k_liner(category_embs)\n",
    "\n",
    "        # print(\"category_q_last\", category_q.shape)\n",
    "        # print(\"category_k_last\", category_k.shape)\n",
    "\n",
    "        category_attention_score = category_q.matmul(category_k.permute(0, 2, 1)) / scaler\n",
    "        category_attention_score = self.dropout(category_attention_score)\n",
    "\n",
    "        final_category_attention_score = category_attention_score[:, 0, :]\n",
    "\n",
    "        category_attention_score = category_attention_score.matmul(category_embs) # [batch_size, max_length, max_length]\n",
    "        category_attention_score = torch.cat([sequence_output, category_attention_score], dim=-1)\n",
    "\n",
    "        # print(category_attention_score.shape)\n",
    "      \n",
    "        logits = self.labels_classifier(category_attention_score)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "class RoBertaBaseClassifier(nn.Module):\n",
    "    def __init__(self, num_label, len_tokenizer):\n",
    "        super(RoBertaBaseClassifier, self).__init__()\n",
    "\n",
    "        self.num_label = num_label\n",
    "        self.electra = ElectraModel.from_pretrained(base_model)\n",
    "        self.electra.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.labels_classifier = SimpleClassifier(self.num_label)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        # print(sequence_output.shape)\n",
    "        logits = self.labels_classifier(sequence_output)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xu_9i_aoreS"
   },
   "source": [
    "## 데이터 파싱 및 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fA7HcyNWq8ui"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
    "\n",
    "    entity_property_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': [],\n",
    "        'category': []\n",
    "    }\n",
    "    polarity_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    for pair in entity_property_pair:\n",
    "        isPairInOpinion = False\n",
    "        if pd.isna(form):\n",
    "            break\n",
    "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
    "        for annotation in annotations:\n",
    "            entity_property = annotation[0]\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if polarity == '------------':\n",
    "                continue\n",
    "\n",
    "            category_seq = [ i for i in range(len(entity_property_pair))]    \n",
    "\n",
    "            if entity_property == pair:\n",
    "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
    "                entity_property_data_dict['category'].append(category_seq)\n",
    "                # print(entity_property_data_dict)\n",
    "\n",
    "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
    "\n",
    "                isPairInOpinion = True\n",
    "                break\n",
    "\n",
    "        if isPairInOpinion is False:\n",
    "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
    "            entity_property_data_dict['category'].append(category_seq) # 개체 없음(즉, 틀린 개체#속성 pair의 경우)\n",
    "\n",
    "    return entity_property_data_dict, polarity_data_dict\n",
    "\n",
    "\n",
    "def get_dataset(raw_data, tokenizer, max_len):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_labels_list = []\n",
    "    category_seq_list = []\n",
    "\n",
    "    polarity_input_ids_list = []\n",
    "    polarity_attention_mask_list = []\n",
    "    polarity_token_labels_list = []\n",
    "\n",
    "    for utterance in raw_data:\n",
    "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
    "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
    "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
    "        token_labels_list.extend(entity_property_data_dict['label'])\n",
    "        category_seq_list.extend(entity_property_data_dict['category'])\n",
    "       \n",
    "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
    "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
    "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
    "\n",
    "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
    "                         torch.tensor(token_labels_list), torch.tensor(category_seq_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
    "                         torch.tensor(polarity_token_labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmGcKTDLrD5-"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oV8CmkrCrJAC"
   },
   "outputs": [],
   "source": [
    "def evaluation(y_true, y_pred, label_len):\n",
    "    count_list = [0]*label_len\n",
    "    hit_list = [0]*label_len\n",
    "    for i in range(len(y_true)):\n",
    "        count_list[y_true[i]] += 1\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            hit_list[y_true[i]] += 1\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(label_len):\n",
    "        acc_list.append(hit_list[i]/count_list[i])\n",
    "\n",
    "    print(count_list)\n",
    "    print(hit_list)\n",
    "    print(acc_list)\n",
    "    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n",
    "    print('macro_accuracy: ', sum(acc_list) / 3)\n",
    "    # print(y_true)\n",
    "\n",
    "    y_true = list(map(int, y_true))\n",
    "    y_pred = list(map(int, y_pred))\n",
    "\n",
    "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
    "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
    "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "def train_sentiment_analysis():\n",
    "\n",
    "    print('train_sentiment_analysis')\n",
    "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
    "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
    "\n",
    "    print('loading train data...')\n",
    "    train_data = jsonlload(train_data_path)\n",
    "    dev_data = jsonlload(dev_data_path)\n",
    "\n",
    "    print('tokenizing train data...')\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "    print('making dataset...')\n",
    "    entity_property_train_data, polarity_train_data = get_dataset(train_data, tokenizer, max_len)\n",
    "    entity_property_dev_data, polarity_dev_data = get_dataset(dev_data, tokenizer, max_len)\n",
    "\n",
    "    print('making dataloader...')\n",
    "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
    "                                  batch_size=batch_size)\n",
    "    entity_property_dev_dataloader = DataLoader(entity_property_dev_data, shuffle=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "    polarity_dev_dataloader = DataLoader(polarity_dev_data, shuffle=True,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print('loading model...')\n",
    "    # entity_property_model = RoBertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
    "    entity_property_model = CategoryClassification(len(tf_id_to_name), len(tokenizer), lstm_hidden * 2, len(entity_property_pair), lstm_num_layer, bilstm_flag)\n",
    "    entity_property_model.to(device)\n",
    "\n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.to(device)\n",
    "\n",
    "    print('end loading')\n",
    "\n",
    "    # entity_property_model_optimizer_setting\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        entity_property_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
    "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
    "\n",
    "    entity_property_optimizer = AdamW(\n",
    "        entity_property_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(entity_property_train_dataloader)\n",
    "    print(\"total_steps : \", total_steps)\n",
    "\n",
    "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
    "        entity_property_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # polarity_model_optimizer_setting\n",
    "    if FULL_FINETUNING:\n",
    "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        polarity_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
    "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
    "\n",
    "    polarity_optimizer = AdamW(\n",
    "        polarity_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(polarity_train_dataloader)\n",
    "\n",
    "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
    "        polarity_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    epoch_step = 0\n",
    "    print(\"학습을 시작합니다...\")\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        entity_property_model.train()\n",
    "        epoch_step += 1\n",
    "\n",
    "        # entity_property train\n",
    "        entity_property_total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(entity_property_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels, b_category_seq = batch\n",
    "            # print(\"b_input_ids:\", b_input_ids)\n",
    "            # print(\"b_input_mask:\", b_input_mask)\n",
    "            # print(\"b_labels:\", b_labels)\n",
    "            # print(\"\")\n",
    "\n",
    "            entity_property_model.zero_grad()\n",
    "\n",
    "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels, b_category_seq)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            entity_property_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
    "            entity_property_optimizer.step()\n",
    "            entity_property_scheduler.step()\n",
    "\n",
    "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = category_extraction_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            entity_property_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in entity_property_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels, b_category_seq = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = entity_property_model(b_input_ids, b_input_mask, b_labels, b_category_seq)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(tf_id_to_name))\n",
    "\n",
    "\n",
    "        # polarity train\n",
    "        polarity_total_loss = 0\n",
    "        polarity_model.train()\n",
    "\n",
    "        for step, batch in enumerate(polarity_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            polarity_model.zero_grad()\n",
    "\n",
    "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            polarity_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
    "            polarity_optimizer.step()\n",
    "            polarity_scheduler.step()\n",
    "\n",
    "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = polarity_classification_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            polarity_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in polarity_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(polarity_id_to_name))\n",
    "\n",
    "    print(\"training is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "q85k-NgDrLo9",
    "outputId": "61d00fd1-e732-47f8-c08e-3c9a77b09a02",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentiment_analysis\n",
      "category_extraction model would be saved at  ./saved_model/category_extraction_emb/\n",
      "polarity model would be saved at  ./saved_model/polarity_classification_emb/\n",
      "loading train data...\n",
      "tokenizing train data...\n",
      "We have added 8 tokens\n",
      "making dataset...\n",
      "making dataloader...\n",
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end loading\n",
      "total_steps :  187580\n",
      "학습을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Property_Epoch:  1\n",
      "Average train loss: 0.14129454995150015\n",
      "[3003, 66847]\n",
      "[1536, 66147]\n",
      "[0.5114885114885115, 0.9895283258784987]\n",
      "accuracy:  0.9689763779527559\n",
      "macro_accuracy:  0.5003389457890034\n",
      "f1_score:  [0.58637144 0.9838838 ]\n",
      "f1_score_micro:  0.9689763779527559\n",
      "f1_score_macro:  0.7851276238352898\n",
      "Entity_Property_Epoch:  1\n",
      "Average train loss: 0.3314095962001011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [21:27<6:47:41, 1287.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2921, 0, 0]\n",
      "[1.0, 0.0, 0.0]\n",
      "accuracy:  0.9726939726939727\n",
      "macro_accuracy:  0.3333333333333333\n",
      "f1_score:  [0.986158 0.       0.      ]\n",
      "f1_score_micro:  0.9726939726939727\n",
      "f1_score_macro:  0.3287193337834797\n",
      "Entity_Property_Epoch:  2\n",
      "Average train loss: 0.10841248687699695\n",
      "[3003, 66847]\n",
      "[1924, 65971]\n",
      "[0.6406926406926406, 0.986895447813664]\n",
      "accuracy:  0.9720114531138153\n",
      "macro_accuracy:  0.542529362835435\n",
      "f1_score:  [0.66310529 0.98539922]\n",
      "f1_score_micro:  0.9720114531138153\n",
      "f1_score_macro:  0.8242522575721528\n",
      "Entity_Property_Epoch:  2\n",
      "Average train loss: 0.1924014145741239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [42:55<6:26:22, 1287.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2921, 0, 0]\n",
      "[1.0, 0.0, 0.0]\n",
      "accuracy:  0.9726939726939727\n",
      "macro_accuracy:  0.3333333333333333\n",
      "f1_score:  [0.986158 0.       0.      ]\n",
      "f1_score_micro:  0.9726939726939727\n",
      "f1_score_macro:  0.3287193337834797\n",
      "Entity_Property_Epoch:  3\n",
      "Average train loss: 0.09367765388483991\n",
      "[3003, 66847]\n",
      "[1815, 66280]\n",
      "[0.6043956043956044, 0.9915179439615839]\n",
      "accuracy:  0.9748747315676449\n",
      "macro_accuracy:  0.5319711827857294\n",
      "f1_score:  [0.67409471 0.9869337 ]\n",
      "f1_score_micro:  0.9748747315676449\n",
      "f1_score_macro:  0.8305142040750048\n",
      "Entity_Property_Epoch:  3\n",
      "Average train loss: 0.1602341804234311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [1:04:25<6:05:09, 1288.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2915, 13, 0]\n",
      "[0.9979459089352961, 0.4642857142857143, 0.0]\n",
      "accuracy:  0.975024975024975\n",
      "macro_accuracy:  0.48741054107367016\n",
      "f1_score:  [0.98796814 0.50980392 0.        ]\n",
      "f1_score_micro:  0.975024975024975\n",
      "f1_score_macro:  0.4992573541872265\n",
      "Entity_Property_Epoch:  4\n",
      "Average train loss: 0.08384895147162572\n",
      "[3003, 66847]\n",
      "[1948, 66111]\n",
      "[0.6486846486846487, 0.9889897826379643]\n",
      "accuracy:  0.9743593414459556\n",
      "macro_accuracy:  0.5458914771075377\n",
      "f1_score:  [0.68507122 0.98663562]\n",
      "f1_score_micro:  0.9743593414459556\n",
      "f1_score_macro:  0.8358534199769672\n",
      "Entity_Property_Epoch:  4\n",
      "Average train loss: 0.14120191658148543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [1:25:52<5:43:29, 1288.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2904, 20, 0]\n",
      "[0.9941800753166724, 0.7142857142857143, 0.0]\n",
      "accuracy:  0.9736929736929737\n",
      "macro_accuracy:  0.5694885965341289\n",
      "f1_score:  [0.9877551  0.55555556 0.        ]\n",
      "f1_score_micro:  0.9736929736929737\n",
      "f1_score_macro:  0.5144368858654573\n",
      "Entity_Property_Epoch:  5\n",
      "Average train loss: 0.07254611953779325\n",
      "[3003, 66847]\n",
      "[1903, 66185]\n",
      "[0.6336996336996337, 0.9900967881879515]\n",
      "accuracy:  0.9747745168217609\n",
      "macro_accuracy:  0.5412654739625284\n",
      "f1_score:  [0.68354885 0.98686369]\n",
      "f1_score_micro:  0.9747745168217609\n",
      "f1_score_macro:  0.8352062685462356\n",
      "Entity_Property_Epoch:  5\n",
      "Average train loss: 0.13156062470981852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [1:47:19<5:21:53, 1287.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2905, 21, 0]\n",
      "[0.9945224238274564, 0.75, 0.0]\n",
      "accuracy:  0.9743589743589743\n",
      "macro_accuracy:  0.5815074746091521\n",
      "f1_score:  [0.98792722 0.5915493  0.        ]\n",
      "f1_score_micro:  0.9743589743589743\n",
      "f1_score_macro:  0.5264921730119992\n",
      "Entity_Property_Epoch:  6\n",
      "Average train loss: 0.061765991853791884\n",
      "[3003, 66847]\n",
      "[2013, 66007]\n",
      "[0.6703296703296703, 0.9874339910541984]\n",
      "accuracy:  0.9738010021474588\n",
      "macro_accuracy:  0.5525878871279563\n",
      "f1_score:  [0.6875     0.98632737]\n",
      "f1_score_micro:  0.9738010021474588\n",
      "f1_score_macro:  0.8369136830937509\n",
      "Entity_Property_Epoch:  6\n",
      "Average train loss: 0.12141880097682588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [2:08:44<5:00:15, 1286.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2900, 21, 0]\n",
      "[0.9928106812735364, 0.75, 0.0]\n",
      "accuracy:  0.9726939726939727\n",
      "macro_accuracy:  0.5809368937578455\n",
      "f1_score:  [0.98757024 0.53164557 0.        ]\n",
      "f1_score_micro:  0.9726939726939727\n",
      "f1_score_macro:  0.5064052687655228\n",
      "Entity_Property_Epoch:  7\n",
      "Average train loss: 0.05444903843851038\n",
      "[3003, 66847]\n",
      "[2039, 66019]\n",
      "[0.678987678987679, 0.9876135054677099]\n",
      "accuracy:  0.9743450250536865\n",
      "macro_accuracy:  0.5555337281517962\n",
      "f1_score:  [0.69471891 0.98660988]\n",
      "f1_score_micro:  0.9743450250536865\n",
      "f1_score_macro:  0.8406643939570415\n",
      "Entity_Property_Epoch:  7\n",
      "Average train loss: 0.119792598002241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [2:30:11<4:38:49, 1286.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2894, 21, 3]\n",
      "[0.9907565902088326, 0.75, 0.05555555555555555]\n",
      "accuracy:  0.9716949716949717\n",
      "macro_accuracy:  0.5987707152547961\n",
      "f1_score:  [0.98703956 0.53164557 0.0952381 ]\n",
      "f1_score_micro:  0.9716949716949717\n",
      "f1_score_macro:  0.5379744094320916\n",
      "Entity_Property_Epoch:  8\n",
      "Average train loss: 0.04682100139032272\n",
      "[3003, 66847]\n",
      "[1996, 66102]\n",
      "[0.6646686646686647, 0.9888551468278307]\n",
      "accuracy:  0.9749176807444524\n",
      "macro_accuracy:  0.5511746038321651\n",
      "f1_score:  [0.69498607 0.98692108]\n",
      "f1_score_micro:  0.9749176807444524\n",
      "f1_score_macro:  0.8409535754932543\n",
      "Entity_Property_Epoch:  8\n",
      "Average train loss: 0.10903113144042437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 8/20 [2:51:39<4:17:25, 1287.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2890, 21, 6]\n",
      "[0.9893871961656967, 0.75, 0.1111111111111111]\n",
      "accuracy:  0.9713619713619713\n",
      "macro_accuracy:  0.6168327690922694\n",
      "f1_score:  [0.98752776 0.51219512 0.16901408]\n",
      "f1_score_micro:  0.9713619713619713\n",
      "f1_score_macro:  0.5562456566661089\n",
      "Entity_Property_Epoch:  9\n",
      "Average train loss: 0.04198846133252614\n",
      "[3003, 66847]\n",
      "[1912, 66170]\n",
      "[0.6366966366966367, 0.9898723951710623]\n",
      "accuracy:  0.974688618468146\n",
      "macro_accuracy:  0.5421896772892331\n",
      "f1_score:  [0.68383405 0.9868166 ]\n",
      "f1_score_micro:  0.974688618468146\n",
      "f1_score_macro:  0.835325322110299\n",
      "Entity_Property_Epoch:  9\n",
      "Average train loss: 0.09356571434269427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|████▌     | 9/20 [3:13:05<3:55:54, 1286.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2904, 17, 2]\n",
      "[0.9941800753166724, 0.6071428571428571, 0.037037037037037035]\n",
      "accuracy:  0.9733599733599734\n",
      "macro_accuracy:  0.5461199898321888\n",
      "f1_score:  [0.98792312 0.64150943 0.05405405]\n",
      "f1_score_micro:  0.9733599733599734\n",
      "f1_score_macro:  0.5611622013975128\n",
      "Entity_Property_Epoch:  10\n",
      "Average train loss: 0.03656304037628426\n",
      "[3003, 66847]\n",
      "[1963, 66121]\n",
      "[0.6536796536796536, 0.9891393779825571]\n",
      "accuracy:  0.9747172512526843\n",
      "macro_accuracy:  0.5476063438874036\n",
      "f1_score:  [0.68973999 0.98682168]\n",
      "f1_score_micro:  0.9747172512526843\n",
      "f1_score_macro:  0.8382808341164054\n",
      "Entity_Property_Epoch:  10\n",
      "Average train loss: 0.08324819936446147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 10/20 [3:34:33<3:34:32, 1287.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2901, 17, 8]\n",
      "[0.9931530297843204, 0.6071428571428571, 0.14814814814814814]\n",
      "accuracy:  0.9743589743589743\n",
      "macro_accuracy:  0.5828146783584419\n",
      "f1_score:  [0.98824732 0.65384615 0.19277108]\n",
      "f1_score_micro:  0.9743589743589743\n",
      "f1_score_macro:  0.6116215185019785\n",
      "Entity_Property_Epoch:  11\n",
      "Average train loss: 0.030410986410172338\n",
      "[3003, 66847]\n",
      "[1994, 65996]\n",
      "[0.6640026640026641, 0.9872694361751462]\n",
      "accuracy:  0.9733715103793844\n",
      "macro_accuracy:  0.5504240333926034\n",
      "f1_score:  [0.68194254 0.98610406]\n",
      "f1_score_micro:  0.9733715103793844\n",
      "f1_score_macro:  0.8340232998424092\n",
      "Entity_Property_Epoch:  11\n",
      "Average train loss: 0.0732990476526902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████▌    | 11/20 [3:55:59<3:13:02, 1286.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2899, 17, 9]\n",
      "[0.9924683327627525, 0.6071428571428571, 0.16666666666666666]\n",
      "accuracy:  0.974025974025974\n",
      "macro_accuracy:  0.5887592855240921\n",
      "f1_score:  [0.9882393  0.64150943 0.20930233]\n",
      "f1_score_micro:  0.974025974025974\n",
      "f1_score_macro:  0.6130170213762087\n",
      "Entity_Property_Epoch:  12\n",
      "Average train loss: 0.02808201900538645\n",
      "[3003, 66847]\n",
      "[2021, 65963]\n",
      "[0.672993672993673, 0.9867757715379898]\n",
      "accuracy:  0.9732856120257695\n",
      "macro_accuracy:  0.5532564815105543\n",
      "f1_score:  [0.68415708 0.98605298]\n",
      "f1_score_micro:  0.9732856120257695\n",
      "f1_score_macro:  0.8351050264544266\n",
      "Entity_Property_Epoch:  12\n",
      "Average train loss: 0.06868988945847378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 12/20 [4:17:28<2:51:38, 1287.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2859, 18, 18]\n",
      "[0.9787743923313934, 0.6428571428571429, 0.3333333333333333]\n",
      "accuracy:  0.964035964035964\n",
      "macro_accuracy:  0.6516549561739565\n",
      "f1_score:  [0.98314993 0.66666667 0.26470588]\n",
      "f1_score_micro:  0.964035964035964\n",
      "f1_score_macro:  0.6381741600812723\n",
      "Entity_Property_Epoch:  13\n",
      "Average train loss: 0.02327567319471759\n",
      "[3003, 66847]\n",
      "[1981, 66002]\n",
      "[0.6596736596736597, 0.9873591933819019]\n",
      "accuracy:  0.9732712956335003\n",
      "macro_accuracy:  0.5490109510185205\n",
      "f1_score:  [0.67970492 0.98605374]\n",
      "f1_score_micro:  0.9732712956335003\n",
      "f1_score_macro:  0.8328793309789391\n",
      "Entity_Property_Epoch:  13\n",
      "Average train loss: 0.05718143567602965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|██████▌   | 13/20 [4:38:57<2:30:15, 1287.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2862, 16, 16]\n",
      "[0.9798014378637453, 0.5714285714285714, 0.2962962962962963]\n",
      "accuracy:  0.9637029637029637\n",
      "macro_accuracy:  0.6158421018628709\n",
      "f1_score:  [0.98299845 0.65306122 0.23880597]\n",
      "f1_score_micro:  0.9637029637029637\n",
      "f1_score_macro:  0.6249552163479984\n",
      "Entity_Property_Epoch:  14\n",
      "Average train loss: 0.021142978826761888\n",
      "[3003, 66847]\n",
      "[2043, 65926]\n",
      "[0.6803196803196803, 0.9862222687629961]\n",
      "accuracy:  0.9730708661417323\n",
      "macro_accuracy:  0.5555139830275588\n",
      "f1_score:  [0.68476621 0.98593466]\n",
      "f1_score_micro:  0.9730708661417323\n",
      "f1_score_macro:  0.8353504375160343\n",
      "Entity_Property_Epoch:  14\n",
      "Average train loss: 0.04661028659698786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 14/20 [5:00:24<2:08:46, 1287.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2896, 14, 9]\n",
      "[0.9914412872304006, 0.5, 0.16666666666666666]\n",
      "accuracy:  0.972027972027972\n",
      "macro_accuracy:  0.5527026512990224\n",
      "f1_score:  [0.98721664 0.62222222 0.19148936]\n",
      "f1_score_micro:  0.972027972027972\n",
      "f1_score_macro:  0.6003094064475973\n",
      "Entity_Property_Epoch:  15\n",
      "Average train loss: 0.017704774665180695\n",
      "[3003, 66847]\n",
      "[2005, 66023]\n",
      "[0.6676656676656677, 0.987673343605547]\n",
      "accuracy:  0.973915533285612\n",
      "macro_accuracy:  0.5517796704237382\n",
      "f1_score:  [0.68758573 0.98638958]\n",
      "f1_score_micro:  0.973915533285612\n",
      "f1_score_macro:  0.8369876558375401\n",
      "Entity_Property_Epoch:  15\n",
      "Average train loss: 0.046919974527409064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 15/20 [5:21:50<1:47:16, 1287.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2890, 15, 13]\n",
      "[0.9893871961656967, 0.5357142857142857, 0.24074074074074073]\n",
      "accuracy:  0.9716949716949717\n",
      "macro_accuracy:  0.5886140742069077\n",
      "f1_score:  [0.98735907 0.6122449  0.25242718]\n",
      "f1_score_micro:  0.9716949716949717\n",
      "f1_score_macro:  0.6173437177153591\n",
      "Entity_Property_Epoch:  16\n",
      "Average train loss: 0.016516461799294505\n",
      "[3003, 66847]\n",
      "[1954, 66110]\n",
      "[0.6506826506826506, 0.988974823103505]\n",
      "accuracy:  0.9744309234073013\n",
      "macro_accuracy:  0.546552491262052\n",
      "f1_score:  [0.68633649 0.98667224]\n",
      "f1_score_micro:  0.9744309234073013\n",
      "f1_score_macro:  0.8365043665560776\n",
      "Entity_Property_Epoch:  16\n",
      "Average train loss: 0.044081689404847565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 16/20 [5:43:16<1:25:47, 1286.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2859, 15, 14]\n",
      "[0.9787743923313934, 0.5357142857142857, 0.25925925925925924]\n",
      "accuracy:  0.9617049617049617\n",
      "macro_accuracy:  0.5912493124349795\n",
      "f1_score:  [0.98213672 0.6        0.20895522]\n",
      "f1_score_micro:  0.9617049617049617\n",
      "f1_score_macro:  0.5970306488854252\n",
      "Entity_Property_Epoch:  17\n",
      "Average train loss: 0.01439175738069661\n",
      "[3003, 66847]\n",
      "[1958, 66127]\n",
      "[0.652014652014652, 0.9892291351893129]\n",
      "accuracy:  0.9747315676449535\n",
      "macro_accuracy:  0.5470812624013216\n",
      "f1_score:  [0.68931526 0.98683023]\n",
      "f1_score_micro:  0.9747315676449535\n",
      "f1_score_macro:  0.8380727434813364\n",
      "Entity_Property_Epoch:  17\n",
      "Average train loss: 0.03664976290179766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  85%|████████▌ | 17/20 [6:04:44<1:04:21, 1287.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2865, 15, 14]\n",
      "[0.9808284833960972, 0.5357142857142857, 0.25925925925925924]\n",
      "accuracy:  0.9637029637029637\n",
      "macro_accuracy:  0.5919340094565474\n",
      "f1_score:  [0.98318463 0.6        0.21875   ]\n",
      "f1_score_micro:  0.9637029637029637\n",
      "f1_score_macro:  0.6006448753145733\n",
      "Entity_Property_Epoch:  18\n",
      "Average train loss: 0.013640146190348354\n",
      "[3003, 66847]\n",
      "[1979, 66081]\n",
      "[0.659007659007659, 0.9885409966041857]\n",
      "accuracy:  0.9743736578382247\n",
      "macro_accuracy:  0.5491828852039483\n",
      "f1_score:  [0.68858733 0.986637  ]\n",
      "f1_score_micro:  0.9743736578382246\n",
      "f1_score_macro:  0.8376121695125848\n",
      "Entity_Property_Epoch:  18\n",
      "Average train loss: 0.03900131058209808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 18/20 [6:26:10<42:53, 1286.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2842, 15, 18]\n",
      "[0.9729544676480657, 0.5357142857142857, 0.3333333333333333]\n",
      "accuracy:  0.9573759573759574\n",
      "macro_accuracy:  0.6140006955652282\n",
      "f1_score:  [0.98033805 0.58823529 0.22929936]\n",
      "f1_score_micro:  0.9573759573759574\n",
      "f1_score_macro:  0.5992909015925312\n",
      "Entity_Property_Epoch:  19\n",
      "Average train loss: 0.012146625705917173\n",
      "[3003, 66847]\n",
      "[1991, 66069]\n",
      "[0.663003663003663, 0.9883614821906742]\n",
      "accuracy:  0.9743736578382247\n",
      "macro_accuracy:  0.5504550483981124\n",
      "f1_score:  [0.68988219 0.98663461]\n",
      "f1_score_micro:  0.9743736578382246\n",
      "f1_score_macro:  0.8382583997615956\n",
      "Entity_Property_Epoch:  19\n",
      "Average train loss: 0.030484878067072715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  95%|█████████▌| 19/20 [6:47:37<21:26, 1286.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2856, 15, 17]\n",
      "[0.9777473467990414, 0.5357142857142857, 0.3148148148148148]\n",
      "accuracy:  0.9617049617049617\n",
      "macro_accuracy:  0.609425482442714\n",
      "f1_score:  [0.98245614 0.58823529 0.24113475]\n",
      "f1_score_micro:  0.9617049617049617\n",
      "f1_score_macro:  0.6039420620805247\n",
      "Entity_Property_Epoch:  20\n",
      "Average train loss: 0.011062437473787084\n",
      "[3003, 66847]\n",
      "[2021, 66015]\n",
      "[0.672993672993673, 0.9875536673298727]\n",
      "accuracy:  0.9740300644237652\n",
      "macro_accuracy:  0.5535157801078485\n",
      "f1_score:  [0.69023224 0.98644691]\n",
      "f1_score_micro:  0.9740300644237652\n",
      "f1_score_macro:  0.838339574389106\n",
      "Entity_Property_Epoch:  20\n",
      "Average train loss: 0.033512031805148584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [7:09:03<00:00, 1287.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2921, 28, 54]\n",
      "[2863, 15, 16]\n",
      "[0.9801437863745293, 0.5357142857142857, 0.2962962962962963]\n",
      "accuracy:  0.9637029637029637\n",
      "macro_accuracy:  0.6040514561283704\n",
      "f1_score:  [0.98351082 0.6        0.23880597]\n",
      "f1_score_micro:  0.9637029637029637\n",
      "f1_score_macro:  0.6074389303909856\n",
      "training is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5gk03mSr6T8"
   },
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tUpzE4G4r7kv"
   },
   "outputs": [],
   "source": [
    "def predict_from_korean_form(tokenizer, ce_model, pc_model, data):\n",
    "\n",
    "    ce_model.to(device)\n",
    "    ce_model.eval()\n",
    "    for sentence in data:\n",
    "        form = sentence['sentence_form']\n",
    "        sentence['annotation'] = []\n",
    "        if type(form) != str:\n",
    "            print(\"form type is arong: \", form)\n",
    "            continue\n",
    "        for pair in entity_property_pair:\n",
    "            \n",
    "            tokenized_data = tokenizer(form, pair, padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "            input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
    "            attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
    "            category_seq = [ i for i in range(len(entity_property_pair))] \n",
    "            category_seq = torch.tensor([category_seq]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, ce_logits = ce_model(input_ids, attention_mask, category_label_seq_tensor=category_seq)\n",
    "\n",
    "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
    "\n",
    "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
    "\n",
    "            if ce_result == 'True':\n",
    "                with torch.no_grad():\n",
    "                    _, pc_logits = pc_model(input_ids, attention_mask)\n",
    "\n",
    "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
    "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
    "\n",
    "                sentence['annotation'].append([pair, pc_result])\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH0tDklIr_I1"
   },
   "source": [
    "## F1 score 계산 - 추출 성능 및 전체 성능에 대한 F1 score 따로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OAnHXy1QsEEl"
   },
   "outputs": [],
   "source": [
    "def evaluation_f1(true_data, pred_data):\n",
    "\n",
    "    true_data_list = true_data\n",
    "    pred_data_list = pred_data\n",
    "\n",
    "    ce_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    pipeline_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    for i in range(len(true_data_list)):\n",
    "\n",
    "        # TP, FN checking\n",
    "        is_ce_found = False\n",
    "        is_pipeline_found = False\n",
    "        for y_ano  in true_data_list[i]['annotation']:\n",
    "            y_category = y_ano[0]\n",
    "            y_polarity = y_ano[2]\n",
    "\n",
    "            for p_ano in pred_data_list[i]['annotation']:\n",
    "                p_category = p_ano[0]\n",
    "                p_polarity = p_ano[1]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is True:\n",
    "                ce_eval['TP'] += 1\n",
    "            else:\n",
    "                ce_eval['FN'] += 1\n",
    "\n",
    "            if is_pipeline_found is True:\n",
    "                pipeline_eval['TP'] += 1\n",
    "            else:\n",
    "                pipeline_eval['FN'] += 1\n",
    "\n",
    "            is_ce_found = False\n",
    "            is_pipeline_found = False\n",
    "\n",
    "        # FP checking\n",
    "        for p_ano in pred_data_list[i]['annotation']:\n",
    "            p_category = p_ano[0]\n",
    "            p_polarity = p_ano[1]\n",
    "\n",
    "            for y_ano  in true_data_list[i]['annotation']:\n",
    "                y_category = y_ano[0]\n",
    "                y_polarity = y_ano[2]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is False:\n",
    "                ce_eval['FP'] += 1\n",
    "\n",
    "            if is_pipeline_found is False:\n",
    "                pipeline_eval['FP'] += 1\n",
    "\n",
    "    ce_precision = ce_eval['TP']/(ce_eval['TP']+ce_eval['FP'])\n",
    "    ce_recall = ce_eval['TP']/(ce_eval['TP']+ce_eval['FN'])\n",
    "\n",
    "    ce_result = {\n",
    "        'Precision': ce_precision,\n",
    "        'Recall': ce_recall,\n",
    "        'F1': 2*ce_recall*ce_precision/(ce_recall+ce_precision)\n",
    "    }\n",
    "\n",
    "    pipeline_precision = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FP'])\n",
    "    pipeline_recall = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FN'])\n",
    "\n",
    "    pipeline_result = {\n",
    "        'Precision': pipeline_precision,\n",
    "        'Recall': pipeline_recall,\n",
    "        'F1': 2*pipeline_recall*pipeline_precision/(pipeline_recall+pipeline_precision)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'category extraction result': ce_result,\n",
    "        'entire pipeline result': pipeline_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuiak9l3sGLf"
   },
   "source": [
    "## 테스트 데이터에 대한 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xfKEPk06sIuU"
   },
   "outputs": [],
   "source": [
    "def test_sentiment_analysis():\n",
    "\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    test_data = jsonlload(dev_data_path)\n",
    "\n",
    "    entity_property_test_data, polarity_test_data = get_dataset(test_data, tokenizer, max_len)\n",
    "\n",
    "    entity_property_test_dataloader = DataLoader(entity_property_test_data, shuffle=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    polarity_test_dataloader = DataLoader(polarity_test_data, shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "    \n",
    "    model =  CategoryClassification(len(tf_id_to_name), len(tokenizer), lstm_hidden * 2, len(entity_property_pair), lstm_num_layer, bilstm_flag)\n",
    "    model.load_state_dict(torch.load(test_category_extraction_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "            \n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
    "    polarity_model.to(device)\n",
    "    polarity_model.eval()\n",
    "\n",
    "    pred_data = predict_from_korean_form(tokenizer, model, polarity_model, copy.deepcopy(test_data))\n",
    "\n",
    "    # jsondump(pred_data, './pred_data.json')\n",
    "    # pred_data = jsonload('./pred_data.json')\n",
    "\n",
    "    print('F1 result: ', evaluation_f1(test_data, pred_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Pe79889ksMNX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 result:  {'category extraction result': {'Precision': 0.7505446623093682, 'Recall': 0.6713218577460215, 'F1': 0.708726212926453}, 'entire pipeline result': {'Precision': 0.7357531760435572, 'Recall': 0.6583306268268918, 'F1': 0.6948920123414466}}\n"
     ]
    }
   ],
   "source": [
    "test_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NNjZffvnczK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "docdoca_kisti",
   "language": "python",
   "name": "docdoca_kisti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
